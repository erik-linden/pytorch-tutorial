{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training with ~~Tensorflow~~ PyTorch\n",
    "We will look at how this lab can be done in PyTorch, a deep learning library developed by Facebook. We will focus on the differences to Tensorflow, and not care too much about the actual results.\n",
    "\n",
    "Let's start with Excersie 1. Here we make a simple computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7776.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def exercise_1():\n",
    "    x = torch.Tensor([2])\n",
    "    y = torch.Tensor([3])\n",
    "    op1 = x + y\n",
    "    op2 = x * y\n",
    "    op3 = op2 ** op1\n",
    "    return op3\n",
    "\n",
    "exercise_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the result without any `Session` or calling `.run()`. PyTorch executes all operations immediately, on the line they are written. There is no C++ computation graph build up behind the scenes. This greatly helps debugging, since errors tend to happen on a specific line, and you can break in the debugger to see the exact state of the computations.\n",
    "\n",
    "It this example, we just got back a simple tensor, which is not very useful for gradient decent. To get gradient information, we have to use PyTorch's autodiff facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7776.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def exercise_1_autodiff():\n",
    "    x = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "    y = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "    op1 = x + y\n",
    "    op2 = x * y\n",
    "    op3 = op2 ** op1\n",
    "    return op3\n",
    "\n",
    "OP3 = exercise_1_autodiff()\n",
    "OP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Variable` is a thin wrapper around the tensor object (side-note: tensors can share memory with numpy, which reduces copies). Variables track gradient information and makes it possible to back-propagate gradients from the output to the input, that is, backprop. It does this by function pointers, which build up a graph that we can walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward1 object at 0x7f4001058290> -> <MulBackward1 object at 0x7f40010582d0>\n",
      "<PowBackward1 object at 0x7f4001058290> -> <AddBackward1 object at 0x7f4001058310>\n",
      "<MulBackward1 object at 0x7f40010582d0> -> <AccumulateGrad object at 0x7f4001058290>\n",
      "<MulBackward1 object at 0x7f40010582d0> -> <AccumulateGrad object at 0x7f4001058350>\n",
      "<AddBackward1 object at 0x7f4001058310> -> <AccumulateGrad object at 0x7f4001058290>\n",
      "<AddBackward1 object at 0x7f4001058310> -> <AccumulateGrad object at 0x7f4001058350>\n"
     ]
    }
   ],
   "source": [
    "def _expand_children(_op):\n",
    "    children = [f[0] for f in _op.next_functions]\n",
    "    edges = [(repr(_op), repr(child)) for child in children]\n",
    "\n",
    "    return children, edges\n",
    "\n",
    "def walk_grad_fn_bf(op):\n",
    "    queue, edges = _expand_children(op.grad_fn)\n",
    "    while queue:\n",
    "        op = queue.pop(0)\n",
    "        children, new_edges = _expand_children(op)\n",
    "        queue += children\n",
    "        edges += new_edges\n",
    "\n",
    "    for e in edges:\n",
    "        print e[0].replace(\"\\n\", \" \"), \"->\", e[1].replace(\"\\n\", \" \")\n",
    "        \n",
    "walk_grad_fn_bf(OP3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variable itself has a \"Pow\" that points to a \"Mul\" and an \"Add\", which is expected. And those two operations points to gradient accumulators. Note that it's the same two accumulators that appear twice. This is since \"Mul\" and \"Add\" both depent on both variables, `x` and `y`, and we need to sum the gradient contribution from both \"Mul\" and \"Add\".\n",
    "\n",
    "Also note that we had to say that the two inputs \"requires_grad\". For efficiency, Variables will only propagate gradients if something upstream of them requires gradients. And the default for `requires_grad` is False, since Variable usually wraps the input, which doesn't need gradients, since we don't want to optimize the input, but the network weights.\n",
    "\n",
    "All other exercises involves training networks, so let's set up the machinery we need for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_loop(loader, net, criterion, optimizer):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    for image, target in loader:\n",
    "        image = Variable(image).to(device)\n",
    "        target = Variable(target).to(device)\n",
    "\n",
    "        prediction = net(image)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / float(len(loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_train_loop` is a helper that will train a single epoch. `loader` will provied batches of training samples, `net` is our neural net, `criterion` is our cost function and `optimizer` is, naturally, our optimizer. During training, we will average the loss and report that back.\n",
    "\n",
    "`net.eval()` sets things like dropout and batch normalization to the proper state.\n",
    "\n",
    "There are several things to note here. First we wrap the input in Variable, which is required by the autodiff backend. Then we call `.to(device)`. This transfers the data to the GPU.\n",
    "\n",
    "We then do the forward pass on the net and the criterion, which is self-explanatory. There is no `feed_dict`.\n",
    "\n",
    "More interesting is the `.zero_grad()` call on the optimizer. As we saw in the first example, Variables, like the network weights, have gradient accumulators that can be added to via separate paths through the back-propagation graph. And they don't know when they should be zeroed. By calling `.zero_grad()`, we zero the gradient accumulators for all Variables the optimizer is optimizing on.\n",
    "\n",
    "Calling `.backward()` on the loss computes the gradient for all Variables that `loss` depends on, with respect to `loss`.\n",
    "\n",
    "Finally, we call `.step()` on the optimizer, which will do some sort of optimization step on all Variables it's working on, based on the gradient information that is now stored in those Variables.\n",
    "\n",
    "Accessing `.data` on the loss side-steps the autograd machinery, so that `total_loss` won't become connected to any back-propagation graph.\n",
    "\n",
    "It should be notet that the back-propagation graph is rebuild on each forward pass, so we can add, remove or change components in the model between batches, which is very hard to do in Tensorflow. It also mean you can use normal python control flow, you don't need things like `tf.control dependencies`.\n",
    "\n",
    "That was a helper for training, now we make a similar one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_loop(loader, net):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    for image, target in loader:\n",
    "        with torch.no_grad():\n",
    "            image = Variable(image).to(device)\n",
    "            target = Variable(target).to(device)\n",
    "\n",
    "            prediction = net(image)\n",
    "\n",
    "            _, prediction = torch.max(prediction, dim=1)\n",
    "            correct += torch.sum(prediction == target).item()\n",
    "        \n",
    "    return correct / float(len(loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar to the training loop, but here we say that the input is \"volatile\". This will kill gradient information on any Variable the input touches, which is the whole net. This improves efficiency, since the net doesn't have to store the information necessary for back-propagation. `net.eval()` disables dropout and freezes any batch normalization.\n",
    "\n",
    "Finally we make a function to train for several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def epoch_loop(net, optimizer, epochs):\n",
    "    print net\n",
    "    \n",
    "    data_train = CIFAR10(\"data\", train=True, download=False, transform=ToTensor())\n",
    "    data_test = CIFAR10(\"data\", train=False, download=False, transform=ToTensor())\n",
    "\n",
    "    def loader_factory(_data):\n",
    "        return DataLoader(_data, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = _train_loop(loader_factory(data_train), net, criterion, optimizer)\n",
    "        top1_accuracy = _test_loop(loader_factory(data_test), net)\n",
    "        print(\"Epoch {}: Top-1 = {:.1f}%, Loss = {:.3e}\".format(epoch, 100 * top1_accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting thing here is `DataLoader`. It is a multi-process dataloader provided by PyTorch. It helps you run IO, image decompression and any data augmentation you want in multiple worker threads, batches the results and  feeds the batches to the GPU. `pin_memory` uses special memory that the GPU has faster access to. `transform` is operations the loader does on the raw data. In this case, we just convert it to tensors. But we could use some of torchvision's pre-defined transforms, like random crops and flips. Unlike the method used in the lab description, this will randomize a new sample in each epoch.\n",
    "\n",
    "Usually it is better to inherit from `DataLoader` and write your own data augmentation code, instead of relying on the pre-defined transforms. This is different from Tensorflow, where loaders are build into the graph and you are limited to what Tensorflow is providing. That is actually a recurring schism between PyTorch and Tensorflow. PyTorch uses standard packages, like `multiprocessing` and `pillow`, while Tensorflow builds everything themselfs.\n",
    "\n",
    "Here we only print the training and test errors as text. Facebook recommends a package called `visdom` for visualization. I didn't like it, but there is an excellent TensorBoard wrapper for PyTorch, `tensorboardX`, which exposes almost the complete interface, and TensorBoard is nice.\n",
    "\n",
    "With all that training stuff in place, let's define some nets. I think everything below is quite straight-forward. Note that we also call `.to(device)` on the nets, to move the parameters to the GPU. And we don't specify input size for anything, so if we input something with the wrong size, we will only find out during the forward-pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Net 2:\n",
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 0: Top-1 = 31.4%, Loss = 1.558e-02\n",
      "Epoch 1: Top-1 = 35.0%, Loss = 1.459e-02\n",
      "Epoch 2: Top-1 = 34.5%, Loss = 1.431e-02\n",
      "Epoch 3: Top-1 = 36.9%, Loss = 1.414e-02\n",
      "Epoch 4: Top-1 = 37.6%, Loss = 1.402e-02\n",
      "Epoch 5: Top-1 = 37.8%, Loss = 1.394e-02\n",
      "Epoch 6: Top-1 = 38.1%, Loss = 1.388e-02\n",
      "Epoch 7: Top-1 = 39.2%, Loss = 1.381e-02\n",
      "Epoch 8: Top-1 = 38.6%, Loss = 1.376e-02\n",
      "Epoch 9: Top-1 = 38.9%, Loss = 1.371e-02\n",
      "Epoch 10: Top-1 = 38.3%, Loss = 1.367e-02\n",
      "Epoch 11: Top-1 = 38.3%, Loss = 1.364e-02\n",
      "Epoch 12: Top-1 = 37.6%, Loss = 1.361e-02\n",
      "Epoch 13: Top-1 = 38.6%, Loss = 1.359e-02\n",
      "Epoch 14: Top-1 = 39.1%, Loss = 1.356e-02\n",
      "Epoch 15: Top-1 = 39.3%, Loss = 1.353e-02\n",
      "Epoch 16: Top-1 = 39.9%, Loss = 1.350e-02\n",
      "Epoch 17: Top-1 = 37.7%, Loss = 1.348e-02\n",
      "Epoch 18: Top-1 = 39.0%, Loss = 1.345e-02\n",
      "Epoch 19: Top-1 = 40.1%, Loss = 1.344e-02\n",
      "\n",
      "Running Net 3:\n",
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=3072, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 0: Top-1 = 36.3%, Loss = 1.533e-02\n",
      "Epoch 1: Top-1 = 40.0%, Loss = 1.362e-02\n",
      "Epoch 2: Top-1 = 42.2%, Loss = 1.284e-02\n",
      "Epoch 3: Top-1 = 44.7%, Loss = 1.244e-02\n",
      "Epoch 4: Top-1 = 44.6%, Loss = 1.212e-02\n",
      "Epoch 5: Top-1 = 46.3%, Loss = 1.193e-02\n",
      "Epoch 6: Top-1 = 45.8%, Loss = 1.166e-02\n",
      "Epoch 7: Top-1 = 46.2%, Loss = 1.158e-02\n",
      "Epoch 8: Top-1 = 46.9%, Loss = 1.142e-02\n",
      "Epoch 9: Top-1 = 47.6%, Loss = 1.133e-02\n",
      "Epoch 10: Top-1 = 47.6%, Loss = 1.120e-02\n",
      "Epoch 11: Top-1 = 45.6%, Loss = 1.118e-02\n",
      "Epoch 12: Top-1 = 47.9%, Loss = 1.105e-02\n",
      "Epoch 13: Top-1 = 48.1%, Loss = 1.091e-02\n",
      "Epoch 14: Top-1 = 47.6%, Loss = 1.077e-02\n",
      "Epoch 15: Top-1 = 49.1%, Loss = 1.054e-02\n",
      "Epoch 16: Top-1 = 48.4%, Loss = 1.047e-02\n",
      "Epoch 17: Top-1 = 48.7%, Loss = 1.051e-02\n",
      "Epoch 18: Top-1 = 49.0%, Loss = 1.043e-02\n",
      "Epoch 19: Top-1 = 48.4%, Loss = 1.045e-02\n",
      "\n",
      "Running Net 4:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (3): Flatten()\n",
      "  (4): Linear(in_features=1280, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 0: Top-1 = 41.0%, Loss = 1.417e-02\n",
      "Epoch 1: Top-1 = 46.8%, Loss = 1.205e-02\n",
      "Epoch 2: Top-1 = 49.8%, Loss = 1.131e-02\n",
      "Epoch 3: Top-1 = 51.7%, Loss = 1.075e-02\n",
      "Epoch 4: Top-1 = 53.5%, Loss = 1.034e-02\n",
      "Epoch 5: Top-1 = 53.5%, Loss = 9.983e-03\n",
      "Epoch 6: Top-1 = 56.1%, Loss = 9.706e-03\n",
      "Epoch 7: Top-1 = 56.8%, Loss = 9.456e-03\n",
      "Epoch 8: Top-1 = 56.3%, Loss = 9.232e-03\n",
      "Epoch 9: Top-1 = 57.8%, Loss = 9.068e-03\n",
      "Epoch 10: Top-1 = 57.9%, Loss = 8.870e-03\n",
      "Epoch 11: Top-1 = 58.3%, Loss = 8.701e-03\n",
      "Epoch 12: Top-1 = 58.9%, Loss = 8.576e-03\n",
      "Epoch 13: Top-1 = 58.8%, Loss = 8.409e-03\n",
      "Epoch 14: Top-1 = 59.1%, Loss = 8.332e-03\n",
      "Epoch 15: Top-1 = 60.1%, Loss = 8.198e-03\n",
      "Epoch 16: Top-1 = 59.6%, Loss = 8.115e-03\n",
      "Epoch 17: Top-1 = 59.9%, Loss = 8.005e-03\n",
      "Epoch 18: Top-1 = 60.0%, Loss = 7.912e-03\n",
      "Epoch 19: Top-1 = 60.7%, Loss = 7.785e-03\n"
     ]
    }
   ],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "def exercise_2_to_4():\n",
    "    print \"Running Net 2:\"\n",
    "    net = torch.nn.Sequential(\n",
    "        Flatten(),\n",
    "        torch.nn.Linear(3 * 32 ** 2, 10, bias=True),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-2)\n",
    "    epoch_loop(net, optimizer, epochs=20)\n",
    "    \n",
    "    print \"\\nRunning Net 3:\"\n",
    "    net = torch.nn.Sequential(\n",
    "        Flatten(),\n",
    "        torch.nn.Linear(3 * 32 ** 2, 128, bias=True),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 10, bias=True),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-3, momentum=0.99)\n",
    "    epoch_loop(net, optimizer, epochs=20)\n",
    "    \n",
    "    print \"\\nRunning Net 4:\"\n",
    "    net = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, padding=1, bias=True),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        Flatten(),\n",
    "        torch.nn.Linear(5 * 16 ** 2, 128, bias=True),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 10, bias=True),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    epoch_loop(net, optimizer, epochs=20)\n",
    "    \n",
    "exercise_2_to_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very impressive results for any of the networks. But hopefully you have learned some of the strengths of PyTorch, and how it compares to Tensorflow. Maybe the simplicity of PyTorch makes you think that it is better than Tensorflow, but that is not necessarily true.\n",
    "\n",
    "Since Tensorflow makes a C++ computation graph, it can apply much heavier optimizations to that graph. Those optimizations could be things like fusing batch normalization into adjacent layers, or common-subexpression-elimination. And since you have your computations in a monolithic C++ blob, it is very easy to deploy on servers, which may be important to you. PyTorch on the other hand, is much more geared towards research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
